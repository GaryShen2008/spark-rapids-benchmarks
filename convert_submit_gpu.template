$SPARK_HOME/bin/spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 10G \
  --num-executors 8 \
  --executor-memory 40G \
  --executor-cores 12 \
  --conf spark.task.cpus=1 \
  --conf spark.task.resource.gpu.amount=0.05 \
  --conf spark.rapids.sql.concurrentGpuTasks=2 \
  --conf spark.plugins=com.nvidia.spark.SQLPlugin \
  --conf spark.sql.files.maxPartitionBytes=2g \
  --conf spark.rapids.memory.pinnedPool.size=8g \
  --conf spark.rapids.sql.explain=NOT_ON_GPU \
  --conf spark.rapids.sql.incompatibleOps.enabled=true \
  --conf spark.rapids.sql.variableFloatAgg.enabled=true \
  --conf spark.rapids.sql.csv.read.date.enabled=true \
  --conf spark.rapids.sql.csvTimestamps.enabled=false \
  --conf spark.rapids.sql.csv.read.integer.enabled=true \
  --conf spark.executor.resource.gpu.amount=1 \
  --conf spark.executor.resource.gpu.discoveryScript=./getGpusResources.sh \
  --files $SPARK_HOME/examples/src/main/scripts/getGpusResources.sh \
  --conf spark.executor.resource.gpu.discoveryScript=./getGpusResources.sh \
  --jars $SPARK_RAPIDS_PLUGIN_JAR,$CUDF_JAR \
  --conf spark.executorEnv.YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=quay.io/nvidia/spark:ubuntu18cuda11-0-yarn3 \
  --conf spark.executorEnv.YARN_CONTAINER_RUNTIME_DOCKER_MOUNTS=/etc/passwd:/etc/passwd:ro,/etc/group:/etc/group:ro,/etc/hadoop/conf:/etc/hadoop/conf:ro \
  --conf spark.yarn.appMasterEnv.YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=quay.io/nvidia/spark:ubuntu18cuda11-0-yarn3 \
  --conf spark.yarn.appMasterEnv.YARN_CONTAINER_RUNTIME_DOCKER_MOUNTS=/etc/passwd:/etc/passwd:ro,/etc/group:/etc/group:ro,/etc/hadoop/conf:/etc/hadoop/conf:ro \
  --conf spark.yarn.appMasterEnv.YARN_CONTAINER_RUNTIME_TYPE=docker \
  --conf spark.executorEnv.YARN_CONTAINER_RUNTIME_TYPE=docker \
